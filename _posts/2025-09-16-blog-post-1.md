---
title: "Blog 1: Israel's Use of AI According to Ethics of Care and Utilitarianism"
date: 2025-09-16
permalink: /posts/2025/09/blog-post-1/
tags:
  - israel
  - palestine
  - genocide
  - AI
  - artificial intelligence
---

<style>
	.serif, #serif, h1, h2, h3, h4, h5, h6, h7, h8, h9, p, meta {
		font-family: Georgia, "Times New Roman", Times, serif;
	}
</style>


*Using A.I. to decide what buildings to bomb and who to assasinate might raise your ethical eyebrows. But what if you were a utilitarian? And what might a feminist say?*

**News Article**  
[Israelâ€™s A.I. Experiments in Gaza War Raise Ethical Concerns](https://www.nytimes.com/2025/04/25/technology/israel-gaza-ai.html)

<hr style="background-color: #000; border: none; height: 2px">

### Background
This morning, Israel began its ground assault on Gaza City, which has already taken the lives of hundreds of civilians. As of their invasion, the death toll of Israel's bombardment on the Gaza strip has reached well over 60,000 according to the United Nations. The UN agency UNICEF has found that well over ten percent of children suffer from acute malnutrition; they also say that number is rising quickly. Yet, despite the obvious humanitarian disaster unfolding throughout Palestine and the resulting international condemnation, Israel has made it clear today that they are not interested in backing down now.

These are not the actions of a sane regime that feels any sort of comfort in their geopolitical position. No, it appears that Israel is sorely afraid; after all, it is a colonial outpost far from its allies and surrounded by nations that are, for one reason or another, hostile to it. How does Israel continue to abuse an ethnicity that constitutes the majority in every surrounding country? What role does technology play in this aggression, and what might be the ethical and material fallout? These are the questions I chose to investigate by selecting an article on the use of artificial intelligence by Israel in military operations in and around the Gaza strip.

### Ethical Concerns and Stakeholders
Clearly, a major ethical concern with using artificial intelligence to aid in military operations is the civilian death toll. In theory, it could be argued that the use of AI allows for a higher degree of precision, minimizing the impact on civilian lives, but in practice, Israel does not appear to be using it in this manner. The article highlights an instance where the Israel was unable to locate Ibrahim Biari, a key figure in the planning of the October 7th attacks; after using AI to pinpoint his approximate location, they bombed several buildings in the area, successfully eliminating him and at least 125 civilians. 

Another issue is the lack of testing for this technology. The article declares that "Israel has used the war in Gaza to rapidly test and deploy A.I.-backed military technologies to a degree that had not been seen before," which benefits the United States military as well. When testing AI means including it in decisions on whether or not to bomb a certain building or assassinate a certain individual -- that is to say, when the result is the loss of human life -- it is reasonable to expect more care to be taken in its testing and deployment, even if it takes a little longer.

Thirdly, the article describes how Israel was able to make a ground-breakingly competent Arabic LLM, not due to some break through in natural language processing architecture, but due to the immense amount of data they had acquired over the years in the form of intercepted texts and phone calls. This model has since been employed as apartheid surveillance, not only in Palestine, but also in neighboring countries -- the article specifically mentions Lebanon.

The main stakeholders in this article are the people of Palestine, to which AI may be a threat on their lives and well being, and Israel's military, for whom AI is a valuable asset in accomplishing security and warfare objectives. However, the article makes it clear that the people of surrounding Arab nations are also in the crosshairs; likewise, Israel's greatest ally, the United States, stands to gain a lot from the rapid development of military artificial intelligence, which is often a collaborative effort between Israeli intelligence agencies and large American tech firms. Other stakeholders include the people of Israel who depend on the IDF for their security, big tech companies like Google and Microsoft, and reporters and humanitarian workers currently in Gaza.

### Military AI According to Ethics of Care and Utilitarianism

#### Ethics of Care
Ethics of care prioritizes interpersonal relationships, which exist between the soldiers who decide to launch missiles and demolish a highrise and the people that live in the targeted building. The introduction of artificial intelligence into this relationship obscures the human element, if not entirely separating the two sides, as machines decide what and who to bomb. Right away, the use of AI to determine what and who to bomb must be considered antithetical to the foundation of this framework. 

What about surveillance? Violating the privacy of individuals is certainly "wrong" according to care ethics, but it could be argued that the responsibility of Israeli officials to care for their constituents supercedes their responsibility to care for the Arabs that live next door. This contradiction is easily resolved: the principle of care ethics is that showing benevolence and care to one's neighbors will most often lead to the reciprocation of that care, so respecting the privacy and well-being of others would in fact be Israel's best option for securing the safety of the Israeli people. 

#### Utilitarianism
I believe that putting this issue onto the framework of utilitarianism exposes some key ethical issues of the broader conflict. Simply put, using artificial intelligence to survey and to kill may have certain negative impacts on the world, but the point to be argued is whether or not those impacts lead to a net improvement for the majority of people. Further complicating this is the ethnic divide, where one choice is vastly preferred for the people of Israel, and the other benefits the lives of Palestinians more. A utilitarian would prefer that both sides get an equal amount of benefit from whatever course of action is taken on using A.I. in this context. 

So, does A.I. benefit more people by increasing the precision and ability to eliminate threats of the Israeli military, or does it do more harm than good by leading to the deaths of innocents and enforcing an apartheid regime? Well, according to the utilitarian, this is a simple numbers game. The Israeli military is already far better equipped to devestate Gaza than any force in Gaza is equipped to harm Israelis. This is why Israel has been able to slaughter more than 65,000 innocent people since the October 7th attacks, which took the lives of 1,200 Israelis. So, even when ignoring Israel's oppressive colonial history and continued aggression against other Arab nations in the Middle East, as any good utilitarian would, we can confidently say the utilitarian perspective is that Israel's use of A.I. technology is wrong.

### Reflection
My experience writing this blog was good. All of the technology-related stuff (git, markdown, etc.) went smoothly. I even set up Jekyll on my laptop so I could publish the website locally! The project was, on the whole, personally beneficial; I wanted to learn more about how Israel is using technology in its ongoing genocide of Palestinians, and the article I found was very instructive in that regard. It was also nice being able to write in a more casual style without worrying too much about holding an academic tone or style.